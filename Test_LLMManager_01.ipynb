{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3535dde-1b30-458d-b582-379870acc5d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llm_manager.factory import LLMFactory\n",
    "from llm_manager.exceptions import UnknownProviderError\n",
    "from llm_manager.prompts.prompt_library import system_prompt\n",
    "from llm_manager.reflection import ReflectiveLLMManager\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from argparse import ArgumentParser\n",
    "import logging\n",
    "import json\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66d74177-12ad-4903-8675-5631f2d8444f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
     ]
    },
    {
     "ename": "LLMProviderError",
     "evalue": "Gemini generation failed: 'builtin_function_or_method' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/llm_manager/src/llm_manager/providers/gemini_client.py:81\u001b[39m, in \u001b[36mGeminiClient.generate.<locals>._call_once\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     80\u001b[39m     response = \u001b[38;5;28mself\u001b[39m._client.models.generate_content(\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m         model=\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[32m     82\u001b[39m         contents=prompt,\n\u001b[32m     83\u001b[39m         config=config\n\u001b[32m     84\u001b[39m     )\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mTypeError\u001b[39m: 'builtin_function_or_method' object is not subscriptable",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mLLMProviderError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m reflecton_manager = ReflectiveLLMManager(llm_client=client)\n\u001b[32m     30\u001b[39m llm_config = {\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model, \u001b[33m'\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m2048\u001b[39m}\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m response = \u001b[43mreflecton_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreflect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreflection_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43madversarial\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm_config\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/llm_manager/src/llm_manager/reflection.py:165\u001b[39m, in \u001b[36mReflectiveLLMManager.reflect\u001b[39m\u001b[34m(self, user_query, reflection_strategy, num_iterations, **kwargs)\u001b[39m\n\u001b[32m    159\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    160\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid reflection strategy \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreflection_strategy\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    161\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mValid options: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[s.value\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39ms\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mReflectionStrategy]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    162\u001b[39m     )\n\u001b[32m    164\u001b[39m \u001b[38;5;66;03m# Start with the original query\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m previous_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[38;5;66;03m# Perform iterations of reflection\u001b[39;00m\n\u001b[32m    168\u001b[39m total_output_tokens = previous_response.usage.get(\u001b[33m\"\u001b[39m\u001b[33moutput_tokens\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/llm_manager/src/llm_manager/providers/gemini_client.py:136\u001b[39m, in \u001b[36mGeminiClient.generate\u001b[39m\u001b[34m(self, prompt, max_tokens, temperature, stream, retry, rate_limit, **kwargs)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m rate_limit:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m retry_call(_call_once, retries=retry)\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_call_once\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/llm_manager/src/llm_manager/retry.py:34\u001b[39m, in \u001b[36mretry_call\u001b[39m\u001b[34m(func, retries, backoff, exceptions)\u001b[39m\n\u001b[32m     32\u001b[39m         delay *= \u001b[32m2\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# If we get here, all retries failed\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m last_exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/llm_manager/src/llm_manager/retry.py:25\u001b[39m, in \u001b[36mretry_call\u001b[39m\u001b[34m(func, retries, backoff, exceptions)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m attempt < retries:\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     27\u001b[39m         last_exc = e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/llm_manager/src/llm_manager/providers/gemini_client.py:86\u001b[39m, in \u001b[36mGeminiClient.generate.<locals>._call_once\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     80\u001b[39m     response = \u001b[38;5;28mself\u001b[39m._client.models.generate_content(\n\u001b[32m     81\u001b[39m         model=kwargs.get[\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     82\u001b[39m         contents=prompt,\n\u001b[32m     83\u001b[39m         config=config\n\u001b[32m     84\u001b[39m     )\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LLMProviderError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGemini generation failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m     88\u001b[39m text_content = response.text \u001b[38;5;28;01mif\u001b[39;00m response.text \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m# Extract Usage\u001b[39;00m\n",
      "\u001b[31mLLMProviderError\u001b[39m: Gemini generation failed: 'builtin_function_or_method' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "provider_name = \"gemini\"\n",
    "params = {\"provider_name\": provider_name}\n",
    "model = \"nemotron-mini\"\n",
    "query = \"How do I make steamed chicken momo (nepali dumplings)?\"\n",
    "\n",
    "if provider_name == \"openai\":\n",
    "    params[\"api_key\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "    model = \"gpt-4o-mini\"\n",
    "elif provider_name == \"anthropic\":\n",
    "    params[\"api_key\"] = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "elif provider_name == \"bedrock\":\n",
    "    params[\"aws_access_key_id\"] = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "    params[\"aws_secret_access_key\"] = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "    params[\"region_name\"] = os.getenv(\"AWS_REGION\")\n",
    "    model = \"anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
    "elif provider_name == \"ollama\":\n",
    "    params[\"base_url\"] = os.getenv(\"OLLAMA_BASE_URL\")\n",
    "    model = \"nemotron-mini\"\n",
    "elif provider_name == \"gemini\":\n",
    "    params[\"api_key\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    model = \"gemini-3-pro-preview\"\n",
    "\n",
    "\n",
    "else:\n",
    "    raise UnknownProviderError(f\"Unsupported provider: {provider_name}\")\n",
    "\n",
    "params[\"system_prompt\"] = system_prompt\n",
    "client = LLMFactory.get_client(**params)\n",
    "reflecton_manager = ReflectiveLLMManager(llm_client=client)\n",
    "llm_config = {\"model\": model, 'max_tokens': 2048}\n",
    "\n",
    "response = reflecton_manager.reflect(\n",
    "    user_query=query,\n",
    "    reflection_strategy=\"adversarial\",\n",
    "    num_iterations=3,\n",
    "    kwargs = llm_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eaca68-1ae1-4f1f-9626-a3bc6644b994",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_dictionary = response.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebe1572-b403-4f3a-87b0-f488a29a02e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = response_dictionary.get('iterations')\n",
    "for i, iteration in enumerate(iterations):\n",
    "    print(f\"Step: {i+1}\\n\")\n",
    "    print(f\"Prompt: {iteration.get('prompt')}\\n\\n\")\n",
    "    print(f\"Response: {iteration.get('response')}\\n\\n\")\n",
    "\n",
    "##print(response_dictionary.get('final_response'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536b431c-9416-42cb-a05d-fdd5b13b69de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddd3647-71f8-4f4a-a857-faf0db9bfbad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_manager",
   "language": "python",
   "name": "llm_manager"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
